import subprocess

import torch
torch.cuda.empty_cache()  # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë©”ëª¨ë¦¬ í•´ì œ


# Step 1: VideoLLaMA3 ì‹¤í–‰
print("\nğŸš€ Running VideoLLaMA3 to generate music genre tags...")
result = subprocess.run(
    ["python", "/home/jaegun/projects/knu_project/VideoLLaMA3/inference/250310_example1.py"], 
    capture_output=True, text=True
)
print("result: ", result)
# VideoLLaMA3 ì¶œë ¥ê°’ì—ì„œ genre íƒœê·¸ ê°€ì ¸ì˜¤ê¸°
generated_tags = result.stdout.strip().split("\n")[-1]
print(f"\ngenerated_tags:{generated_tags}")



# Step 2: YuE ì‹¤í–‰ (VideoLLaMA3 ê²°ê³¼ë¥¼ ì „ë‹¬)

# YuE ì‹¤í–‰ ê²½ë¡œ
yue_inference_path = "/home/jaegun/projects/knu_project/YuE/inference"

print("\nğŸš€ Running YuE to generate background music...")
yue_cmd = [
    "python", "250310_infer_copy.py",
    "--genre", generated_tags,
    "--lyrics_txt", "/home/jaegun/projects/knu_project/YuE/prompt_egs/lyrics.txt",
    "--cuda_idx", "0",
    "--stage1_model", "m-a-p/YuE-s1-7B-anneal-en-cot",
    "--stage2_model", "m-a-p/YuE-s2-1B-general",
    "--run_n_segments", "2",
    "--stage2_batch_size", "4",
    "--output_dir", "/home/jaegun/projects/knu_project/main/output",
    "--max_new_tokens", "3000",
    "--repetition_penalty", "1.1"
]

# YuE ì‹¤í–‰ ì‹œ cwd ì§€ì • (ì‹¤í–‰ ê²½ë¡œ ë³€ê²½)
result = subprocess.run(yue_cmd, cwd=yue_inference_path, capture_output=True, text=True)
